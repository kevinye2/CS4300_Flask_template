{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many public datasets for Reddit, they are often years old or not updated freqently. The goal of the project is to provide information on Covid-19 and related legal advice, so we're dealing with rapidly developing news and would like to have access to recent activity, as well as historical Reddit posts in a specified period of time. The Reddit API (via PRAW as a Python Reddit API Wrapper, making implementation easier) is powerful and can provide a lot of data and metadata, however it is limited in scope for time periods and response size. To address that, we can use the Pushshift API, an API that provides access to a vast amount of public Reddit data that has been ammassed over the years and continues to get updated by the day (? pretty frequenly I'm pretty sure). Another option rather that is accessing the back-end Elasticsearch search engine endpoint (but doesn't work right now?).\n",
    "\n",
    "Prototype 1: For the moment, Reddit data will use PRAW, which is relatively easy to retrieve. There is an Reddit Api call that can search within a subreddit, which utilizes a cloud-based search engine backend (like Apache Lucene) for internal site searches, but we'd be relying too much on Reddit's relevance algorithms. (There are other Reddit Api functions that get the top/new/hot/controversial posts in a subreddit, but they don't search queries. We could do this, but once again, the Reddit Api is limited in retrieval/time limit, so this would only be for special cases). \n",
    "Since to get historic data via Pushshift takes a while and we need to decide how to process a lot of data quickly, we won't use that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import praw\n",
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "# from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pushshift API\n",
    "\n",
    "Note:\n",
    "Includes deleted posts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max batch size 1000\n",
    "# time range field m doesn't work...?\n",
    "# Retrieving top scored posts from the Coronavirus subreddit from 120 days ago\n",
    "top = 'https://api.pushshift.io/reddit/search/submission/?subreddit=coronavirus&sort_type=score'\n",
    "\n",
    "res = requests.get(top)\n",
    "data_arr = res.json()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr\n",
    "for k, obj in enumerate(data_arr):\n",
    "    print(20*'-', k)\n",
    "    print('Title:', obj['title'])\n",
    "    print('Author:', obj['author'])\n",
    "    print(obj['full_link'])\n",
    "    print('Score:', obj['score'])\n",
    "    epoch = obj['created_utc']\n",
    "    print('Epoch:', epoch)\n",
    "    print('Date:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(epoch)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = os.environ['CLIENT_ID'],\n",
    "                     client_secret = os.environ['CLIENT_SECRET'],\n",
    "                     user_agent = 'redditdata')\n",
    "\n",
    "subreddit = reddit.subreddit('all')\n",
    "top_coronavirus = subreddit.top(limit=None) # 1000 max...?\n",
    "for k, submission in enumerate(top_coronavirus):\n",
    "    print(20*'-', k)\n",
    "    print(submission.title) # The title of the submission.\n",
    "    print(submission.score) # The number of upvotes for the submission.\n",
    "#     print(submission.id) # ID of the submission\n",
    "#     print(submission.permalink) # A permalink for the submission.\n",
    "#     print(submission.selftext) # The submissions’ selftext - an empty string if a link post.\n",
    "#     print(submission.url) # The URL the submission links to, or the permalink if a selfpost.\n",
    "#     print(submission.upvote_ratio) # The percentage of upvotes from all votes on the submission.\n",
    "\n",
    "    epoch = submission.created_utc\n",
    "    print('Epoch:', epoch)\n",
    "    print('Date:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(epoch)))\n",
    "    # all_reddit.append((submission.title, submission.selftext , submission.id, reddit_base_url + submission.permalink)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Reddit's search algorithm from its listings (Reddit's data structure for the list of ranked posts.). It utilizes stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters for searching a query in a subreddit.\n",
    "# query – The query string to search for.\n",
    "# sort – Can be one of: relevance, hot, top, new, comments. (default: relevance).\n",
    "# syntax – Can be one of: cloudsearch, lucene, plain (default: lucene).\n",
    "# time_filter – Can be one of: all, day, hour, month, week, year (default: all).\n",
    "# TODO: Work on getting other subreddits like CoronavirusNewYork; or filter an extra field, flair=New York, in the Coronavirus subreddit\n",
    "for k, submission in enumerate(reddit.subreddit('all').search('coronavirus', limit = 1000)):\n",
    "    print(20*'-', k)\n",
    "    print(submission.title)\n",
    "    print(submission.score)\n",
    "    epoch = submission.created_utc\n",
    "    print('Epoch:', epoch)\n",
    "    print('Date:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(epoch)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers = {'User-agent': 'redditRetrival'}, needed to uniquely identify to prevent rate limiting\n",
    "res = requests.get('https://www.reddit.com/r/all/search/.json?q=coronavirus&restrict_sr=1&limit=1000', headers = {'User-agent': 'redditRetrival'})\n",
    "print(res)\n",
    "vv = res.json()\n",
    "vv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the 'public' Reddit API, aka through the normal search bar web interface, with .json extension. \n",
    "\n",
    "Advantage of this over PRAW is that there is no (rather insignificant) limit rate on requests, as well as request size limit. However, it can only pull 100, regardless of if it is search in subreddit or just the top/hot ranked in a subreddit. (But we probably won't need more than 100?)\n",
    "\n",
    "https://www.reddit.com/r/all/search/.json?q=coronavirus&restrict_sr=1&limit=1000 \\\n",
    "https://www.reddit.com/r/Coronavirus/top/.json?t=all&limit=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submissions_from_pushshift(**kwargs):\n",
    "    '''\n",
    "    Use Pushshift API to pull in batches (max 1000).\n",
    "    \n",
    "    Use parameters from:\n",
    "    https://github.com/pushshift/api\n",
    "    \n",
    "    By default, results are returned based on created_utc in descending order, but if in ascending order\n",
    "    if before/after is specified.  Must specify sort_order in that case if want time-window results descending.\n",
    "    \n",
    "    parameters:\n",
    "        subreddit: subreddit to pull results\n",
    "        after: Return results after this date\n",
    "        before: Return results before this date\n",
    "    \n",
    "    returns:\n",
    "        list of subreddit data\n",
    "    '''\n",
    "    r = requests.get(\"https://api.pushshift.io/reddit/submission/search/\", params=kwargs)\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            data = r.json()\n",
    "            break\n",
    "        except:\n",
    "            print(r)\n",
    "        time.sleep(2)\n",
    "    return data['data']\n",
    "\n",
    "def get_submissions_from_reddit_api(submission_ids):\n",
    "    '''\n",
    "    Use the Reddit API to get most updated metadata, such as scores. Useful for most up to data text & scores, \n",
    "    but not necessary for data collection\n",
    "    '''\n",
    "    headers = {'User-agent':'Submission Collector'}\n",
    "    params = {}\n",
    "    params['id'] = ','.join([\"t3_\" + id for id in submission_ids])\n",
    "    r = requests.get(\"https://api.reddit.com/api/info/\", params=params, headers=headers)\n",
    "    data = r.json()\n",
    "    return data['data']['children']\n",
    "\n",
    "\n",
    "def get_all_submissions_from_subreddit(subreddit, after=None, before=None):   \n",
    "    allSubredditData = [] # Collect all data\n",
    "    \n",
    "    # Loop in batches\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        submissions = get_submissions_from_pushshift(subreddit=subreddit, size=1000, after=after, before=before)\n",
    "        end_time = time.time()\n",
    "        print(\"Retrieved {} results in {:.4f}s. Total results so far: {}\".format(len(submissions), end_time - start_time, len(allSubredditData)))\n",
    "        allSubredditData.extend(submissions)\n",
    "        \n",
    "        if not submissions: \n",
    "            break\n",
    "\n",
    "        # This will get the submission ids from Pushshift in batches of 1000 -- Reddit's API only allows 100 at a time\n",
    "        submission_ids = []\n",
    "        for submission in submissions:\n",
    "            after = submission['created_utc'] # This will keep track of your position for the next call in the while loop\n",
    "            submission_ids.append(submission['id'])\n",
    "    #     print(\"first:\", submissions[0]['created_utc'])\n",
    "        print(\"after:\", after)\n",
    "        epoch = submissions[-1]['created_utc']\n",
    "#         print('Epoch:', epoch)\n",
    "        print('Last Date:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(epoch)))\n",
    "\n",
    "    #     This will then pass the ids collected from Pushshift and query Reddit's API for the most up to date information\n",
    "    #     submissions = get_submissions_from_reddit_api(submission_ids)\n",
    "    #     for submission in submissions:\n",
    "    #         submission = submission['data']\n",
    "            # Do stuff with the submissions\n",
    "    #         print(submission['score'], submission['url'], submission['title'])\n",
    "    #     print()\n",
    "\n",
    "    #     time.sleep(2) # I'm not sure how often you can query the Reddit API without oauth but once every two seconds should work fine\n",
    "    \n",
    "    return allSubredditData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [{'before': None, 'after': int(datetime.datetime(year=2019, month=12, day=1).timestamp()), 'subreddit': \"coronavirus\"},\n",
    "           {'before': None, 'after': int(datetime.datetime(year=2019, month=12, day=1).timestamp()), 'subreddit': \"legaladvice\"}]\n",
    "for query in queries:\n",
    "    data = get_all_submissions_from_subreddit(query['subreddit'], before=query['before'], after=query['after'])\n",
    "    print(len(data))\n",
    "    print(type(data))\n",
    "    with open(query['subreddit'] + '.json', 'w') as f:\n",
    "        json.dump(allSubredditData, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_file = json.load(open('data.json'))\n",
    "# my_json_string = json.dumps(allSubredditData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_json_string\n",
    "reddit_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
