{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many public datasets for Reddit, they are often years old or not updated freqently. The goal of the project is to provide information on Covid-19 and related legal advice, so we're dealing with rapidly developing news and would like to have access to recent activity, as well as historical Reddit posts in a specified period of time. The Reddit API (via PRAW as a Python Reddit API Wrapper, making implementation easier) is powerful and can provide a lot of data and metadata, however it is limited in scope for time periods and response size. To address that, we can use the Pushshift API, an API that provides access to a vast amount of public Reddit data that has been ammassed over the years and continues to get updated by the day (? pretty frequenly I'm pretty sure). Another option rather that is accessing the back-end Elasticsearch search engine endpoint (but doesn't work right now?).\n",
    "\n",
    "Prototype 1: For the moment, Reddit data will use PRAW, which is relatively easy to retrieve. There is an Reddit Api call that can search within a subreddit, which utilizes a cloud-based search engine backend (like Apache Lucene) for internal site searches, but we'd be relying too much on Reddit's relevance algorithms. (There are other Reddit Api functions that get the top/new/hot/controversial posts in a subreddit, but they don't search queries. We could do this, but once again, the Reddit Api is limited in retrieval/time limit, so this would only be for special cases). \n",
    "Since to get historic data via Pushshift takes a while and we need to decide how to process a lot of data quickly, we won't use that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import praw\n",
    "import requests\n",
    "# from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pushshift API\n",
    "\n",
    "Note:\n",
    "Includes deleted posts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max batch size 1000\n",
    "# time range field m doesn't work...?\n",
    "# Retrieving top scored posts from the Coronavirus subreddit from 120 days ago\n",
    "top = 'https://api.pushshift.io/reddit/search/submission/?subreddit=coronavirus&after=120d&size=2000'\n",
    "\n",
    "res = requests.get(top)\n",
    "data_arr = res.json()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr\n",
    "for k, obj in enumerate(data_arr):\n",
    "    print(20*'-', k)\n",
    "    print('Title:', obj['title'])\n",
    "    print('Author:', obj['author'])\n",
    "    print(obj['full_link'])\n",
    "    print('Score:', obj['score'])\n",
    "    epoch = obj['created_utc']\n",
    "    print('Epoch:', epoch)\n",
    "    print('Date:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(epoch)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = os.environ['CLIENT_ID'],\n",
    "                     client_secret = os.environ['CLIENT_SECRET'],\n",
    "                     user_agent = 'redditdata')\n",
    "\n",
    "subreddit = reddit.subreddit('all')\n",
    "top_coronavirus = subreddit.top(limit=None) # 1000 max...?\n",
    "for k, submission in enumerate(top_coronavirus):\n",
    "    print(20*'-', k)\n",
    "    print(submission.title) # The title of the submission.\n",
    "    print(submission.score) # The number of upvotes for the submission.\n",
    "#     print(submission.id) # ID of the submission\n",
    "#     print(submission.permalink) # A permalink for the submission.\n",
    "#     print(submission.selftext) # The submissions’ selftext - an empty string if a link post.\n",
    "#     print(submission.url) # The URL the submission links to, or the permalink if a selfpost.\n",
    "#     print(submission.upvote_ratio) # The percentage of upvotes from all votes on the submission.\n",
    "\n",
    "    epoch = submission.created_utc\n",
    "    print('Epoch:', epoch)\n",
    "    print('Date:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(epoch)))\n",
    "    # all_reddit.append((submission.title, submission.selftext , submission.id, reddit_base_url + submission.permalink)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Reddit's search algorithm from its listings (Reddit's data structure for the list of ranked posts.). It utilizes stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters for searching a query in a subreddit.\n",
    "# query – The query string to search for.\n",
    "# sort – Can be one of: relevance, hot, top, new, comments. (default: relevance).\n",
    "# syntax – Can be one of: cloudsearch, lucene, plain (default: lucene).\n",
    "# time_filter – Can be one of: all, day, hour, month, week, year (default: all).\n",
    "# TODO: Work on getting other subreddits like CoronavirusNewYork; or filter an extra field, flair=New York, in the Coronavirus subreddit\n",
    "for k, submission in enumerate(reddit.subreddit('all').search('coronavirus', limit = 1000)):\n",
    "    print(20*'-', k)\n",
    "    print(submission.title)\n",
    "    print(submission.score)\n",
    "    epoch = submission.created_utc\n",
    "    print('Epoch:', epoch)\n",
    "    print('Date:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(epoch)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers = {'User-agent': 'redditRetrival'}, needed to uniquely identify to prevent rate limiting\n",
    "res = requests.get('https://www.reddit.com/r/all/search/.json?q=coronavirus&restrict_sr=1&limit=1000', headers = {'User-agent': 'redditRetrival'})\n",
    "print(res)\n",
    "vv = res.json()\n",
    "vv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the 'public' Reddit API, aka through the normal search bar web interface, with .json extension. \n",
    "\n",
    "Advantage of this over PRAW is that there is no (rather insignificant) limit rate on requests, as well as request size limit. However, it can only pull 100, regardless of if it is search in subreddit or just the top/hot ranked in a subreddit. (But we probably won't need more than 100?)\n",
    "\n",
    "https://www.reddit.com/r/all/search/.json?q=coronavirus&restrict_sr=1&limit=1000 \\\n",
    "https://www.reddit.com/r/Coronavirus/top/.json?t=all&limit=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
